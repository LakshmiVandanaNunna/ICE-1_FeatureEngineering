b"Self-driving cars are being developed by several major technology companies and carmakers. credit: VCG/Getty\n\nWhen a driver slams on the brakes to avoid hitting a pedestrian crossing the road illegally, she is making a moral decision that shifts risk from the pedestrian to the people in the car. Self-driving cars might soon have to make such ethical judgments on their own \xe2\x80\x94 but settling on a universal moral code for the vehicles could be a thorny task, suggests a survey of 2.3 million people from around the world.\n\nThe largest ever survey of machine ethics1, published today in Nature, finds that many of the moral principles that guide a driver\xe2\x80\x99s decisions vary by country. For example, in a scenario in which some combination of pedestrians and passengers will die in a collision, people from relatively prosperous countries with strong institutions were less likely to spare a pedestrian who stepped into traffic illegally.\n\n\xe2\x80\x9cPeople who think about machine ethics make it sound like you can come up with a perfect set of rules for robots, and what we show here with data is that there are no universal rules,\xe2\x80\x9d says Iyad Rahwan, a computer scientist at the Massachusetts Institute of Technology in Cambridge and a co-author of the study.\n\nThe survey, called the Moral Machine, laid out 13 scenarios in which someone\xe2\x80\x99s death was inevitable. Respondents were asked to choose who to spare in situations that involved a mix of variables: young or old, rich or poor, more people or fewer.\n\nPeople rarely encounter such stark moral dilemmas, and some critics question whether the scenarios posed in the quiz are relevant to the ethical and practical questions surrounding driverless cars. But the study\xe2\x80\x99s authors say that the scenarios stand in for the subtle moral decisions that drivers make every day. They argue that the findings reveal cultural nuances that governments and makers of self-driving cars must take into account if they want the vehicles to gain public acceptance.\n\nAt least one company working on self-driving cars \xe2\x80\x94 the German carmaker Audi \xe2\x80\x94 says that the survey could help prompt an important discussion about these issues. (Other firms with autonomous-vehicle programmes, including auto manufacturer Toyota and technology companies Waymo and Uber, declined to comment on the findings.) And Nicholas Christakis, a social scientist at Yale University in New Haven, Connecticut, is fascinated by the results.\n\n\xe2\x80\x9cIt\xe2\x80\x99s a remarkable paper,\xe2\x80\x9d he says. The debate about whether ethics are universal or vary between cultures is an old one, says Christakis, and now the \xe2\x80\x9ctwenty-first century problem\xe2\x80\x9d of how to programme self-driving cars has reinvigorated it.\n\nThe road not taken\n\nSome of the world\xe2\x80\x99s biggest tech companies \xe2\x80\x94 including Google's parent, Alphabet; Uber; and Tesla \xe2\x80\x94 and carmakers now have self-driving car programmes. Many of these companies argue that the vehicles could improve road safety, ease traffic and improve fuel efficiency. Social scientists say the cars raise ethical issues, and could have unintended consequences for public safety and the environment.\xef\xbb\xbf\n\nIn 2016, Rahwan\xe2\x80\x99s team stumbled on an ethical paradox about self-driving cars2: in surveys, people said that they wanted an autonomous vehicle to protect pedestrians even if it meant sacrificing its passengers \xe2\x80\x94 but also that they wouldn\xe2\x80\x99t buy self-driving vehicles programmed to act this way.\n\nCurious to see if the prospect of self-driving cars might raise other ethical condundrums, Rahwan gathered an international team of psychologists, anthropologists and economists to create the Moral Machine. Within 18 months, the online quiz had recorded 40 million decisions made by people from 233 countries and territories.\n\nNo matter their age, gender or country of residence, most people spared humans over pets, and groups of people over individuals. These responses are in line with rules proposed in what may be the only governmental guidance on self-driving cars: a 2017 report by the German Ethics Commission on Automated and Connected Driving.\n\nBut agreement ends there. When the authors analysed answers from people in the 130 countries with at least 100 respondents, they found that the nations could be divided into three groups. One contains North America and several European nations where Christianity has historically been the dominant religion; another includes countries such as Japan, Indonesia and Pakistan, with strong Confucian or Islamic traditions. A third group consists of Central and South America, as well as France and former French colonies. The first group showed a stronger preference for sacrificing older lives to save younger ones than did the second group, for example.\n\nThe researchers also identified correlations between social and economic factors in a country and the average opinions of its residents. The team found that people from countries with strong government institutions, such as Finland and Japan, more often chose to hit people who were crossing the road illegally than did respondents in nations with weaker institutions, such as Nigeria or Pakistan.\n\nScenarios that forced survey participants to choose whether to save a homeless person on one side of the road or an executive on the other revealed another point of departure: the choices people made often correlated with the level of economic inequality in their culture. People from Finland \xe2\x80\x94 which has a relatively small gap between the rich and the poor \xe2\x80\x94 showed little preference for swerving one way or the other. But the average respondent from Colombia \xe2\x80\x94 a country with significant economic disparity \xe2\x80\x94 chose to kill the lower-status person.\n\nAzim Shariff, a psychologist at the University of British Columbia in Vancouver, finds this result interesting because it suggests that the survey really does reveal people\xe2\x80\x99s moral preferences. \xe2\x80\x9cIf you assume that places that have a lower level of income inequality have political policies that favor egalitarianism, this shows that the moral norms that support those policies are expressed in the way that people play these games.\xe2\x80\x9d\n\nPotholes ahead?\n\nAlthough autonomous vehicles aren't yet for sale to the public, test versions are cruising through several US cities. By 2021, at least five manufacturers hope to have self-driving cars and trucks in wide use.\n\nBryant Walker Smith, a law professor at the University of South Carolina in Columbia, is sceptical that the Moral Machine survey will have any practical use. He says that the study is unrealistic because there are few instances in real life in which a vehicle would face a choice between striking two different types of people. \xe2\x80\x9cI might as well worry about how automated cars will deal with asteroid strikes,\xe2\x80\x9d Walker Smith says.\n\nBut the study\xe2\x80\x99s authors say that their scenarios represent the minor moral judgements that human drivers make routinely \xe2\x80\x94 which can sometimes be fatal. A driver who veers away from cyclists riding on a curvy mountain road increases her chance of hitting an oncoming vehicle. If the number of driverless cars on the road increases, so too will the likelihood that they will be involved in such accidents.\n\nSome car companies are listening. Barbara Wege, who heads a group focused on autonomous-vehicle ethics at Audi in Ingolstadt, Germany, says such studies are valuable. Wege argues that self-driving cars would cause fewer accidents, proportionally, than human drivers do each year \xe2\x80\x94 but that events involving robots might receive more attention.\n\nSurveys such as the Moral Machine can help to begin public discussions about these inevitable accidents that might foster trust. \xe2\x80\x9cWe need to come up with a social consensus,\xe2\x80\x9d she says, \xe2\x80\x9cabout which risks we are willing to take.\xe2\x80\x9d"